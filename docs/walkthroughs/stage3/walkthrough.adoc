:toc:
:toc-placement!:

= Lab 3 - Gitter ⇔ Kafka ⇔ Slack (via streams)

toc::[]

== Overview
Lab 1 and 2 enabled Gitter/Slack conversations. However their connectivity was tightly coupled with dedicated data translations between both platforms. On this third lab we want to break them away to open up the architecture to welcome additional systems and services.

Target persona: +

* *Kubernetes User*

Difficulty level: +

* *EASY*

Estimated time: +

* *20 mn*


{empty} +


The picture billow illustrates an asynchronous decoupled architecture, via a streaming platform (kafka). This approach increases the number of data flows from two to four.

TIP: At first it seems unnecessary to double the number of data flows, but benefits outweighs complexity, we gain an easily extensible architecture. In later labs you'll see new additions to the architecture.

{empty} +

// image::images/processing-flow.png[title="Data flow",align="center",title-align=center, width=80%]
image::images/data-flow.png[align="center", width=90%]

{empty} +

In terms of implementation effort for this lab, your main task is to split each of your current data flows (from Lab-2) in two different parts:

- The Gitter to Slack process into:
. Gitter to Kafka
. Kafka to Slack
- The Slack to Gitter process into:
. Slack to Kafka
. Kafka to Gitter

{empty} +

One fundamental architecture consideration is that if we want an easy to plugin platform where other communication systems or services need to plugin with ease, we should adopt a standard data model. It would establish a common interface for systems willing to integrate with the platform.

This implies that instead of applying platform specific data transformations (eg. Gitter data model to Slack data model), we apply the following data transformations:

- System specific to standard data model (e.g. Gitter/Slack to Kafka)
- Standard data model to System specific (e.g. Kafka to Gitter/Slack)

{empty} +

The illustration below describes data exchanges via Kafka:

image::images/standard-data-model.png[align="center", width=90%]

{empty} +


== Setup

First of all, ensure you eliminate Lab2's deployed bindings, otherwise they will enter in conflict with the ones we're about to create:
```bash
oc delete klb g2s
oc delete klb s2g
```

We continue growing our solution where we left it in Lab 2. +
We use Lab-2 as the base for this next stage.

The following set of instructions prepare the set of files you will be working with: 

. Prepare Lab 3 folder
+
```bash
cp -r lab2 lab3
cd lab3
mv stage2.properties stage3.properties
grep -rl stage2 . | xargs sed -i '' 's/stage2/stage3/g'
 
```
+
. Split each YAML file in two:
+
```bash
mv g2s.yaml g2k.yaml
cp g2k.yaml k2s.yaml
mv s2g.yaml s2k.yaml
cp s2k.yaml k2g.yaml
mkdir flows
mv *.yaml flows/ 
```
+
. Rename the bindings:
+
```bash
sed -i '' 's/g2s/g2k/g' flows/g2k.yaml
sed -i '' 's/g2s/k2s/g' flows/k2s.yaml
sed -i '' 's/s2g/s2k/g' flows/s2k.yaml
sed -i '' 's/s2g/k2g/g' flows/k2g.yaml
```
+
. Prepare JSLTs:
+
```bash
rm *.jslt
mkdir maps
touch maps/g2k.jslt
touch maps/k2s.jslt
touch maps/s2k.jslt
touch maps/k2g.jslt
 
```

You're ready to go.

{empty} +

== Gitter/Slack to Kafka

The two data flows we have created in previous labs are almost identical in terms of processing steps, those are:

. receive events
. filter events
. transform events
. push events

For the processes from Gitter/Slack to Kafka, the steps remain the same, we just need to switch to the standard data model (step 3) and target Kafka instead (step 4).

{empty} +

=== Process overview

The diagram below applies to the data flows (2 of them) from Gitter/Slack respectively to Kafka:

image::images/processing-flow-chat2kafka.png[align="center", width=90%]


* There are 4 Kamelets in use:
+
====
A source::
consumes events from Gitter/Slack.
Two actions::
one filters messages to prevent death loops. +
one transforms Gitter/Slack events to the standard data model.
A sink::
	produces events to Kafka.
====

{empty} +

As in lab 1 & 2, this one also fits the _Kubernetes_ user. We compose the definitions using Kamelets to enable the data flows between the different platforms.

{empty} +

=== Gitter to Kafka

. Replace the sink to target Kafka
+
Open and edit your `g2k.yaml` file.
+
The original definition remains intact except for the sink to be replaced by a _Kafka_ destination. +
Copy the sink snippet down below and paste it in your _Kamelet Binding_:
+
----
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: g2k
  annotations:
    trait.camel.apache.org/mount.configs: "secret:stage3"
    trait.camel.apache.org/mount.resources: "configmap:stage3-transform"
spec:

  source:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: gitter-source
    properties:
      token: "{{gitter.token}}"
      room:  "{{gitter.room}}"

  steps:
    
  # Filter action to prevent death loops
  - ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: predicate-filter-action
    properties:
      expression: $.text =~ /(?!\*\*.*@.*\*\*:).*/

  - ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: jslt-action
    properties:
      template: g2k.jslt
----
+
```yaml
  sink:
    ref:
      kind: KafkaTopic
      apiVersion: kafka.strimzi.io/v1beta1
      name: YOUR_ROOM_NAME
```
+
[IMPORTANT]
====
Ensure you configure the sink's `name` (Kafka topic) by replacing `YOUR_ROOM_NAME` with your chat's room name.
====
+
[NOTE]
====
* The sink definition in a Kamelet Binding can either be a Kamelet Sink from the Catalog, or a platform resource (Kafka or KNative).
* Kafka definitions just require the name of the topic. The Camel K operator automatically wires the connectivity to the Kafka platform available in the environment.
====
{empty} +

. Define the JSLT transformation to the new standard data model.
+
Copy the snippet below and paste it into your new `g2k.jslt` file:
+
```
{
	"timestamp": string(round(parse-time(.sent, "yyyy-MM-dd'T'HH:mm:ss.SSSX"))),
	"source":"gitter", 
	"user": .fromUser.displayName, 
	"text": .text
}
```
+
[NOTE]
====
* We include various fields to provide context.
* We apply a format on the timestamp to match those from other sources.
====
{empty} +

And that's all it takes for this first stint.

{empty} +



=== Slack to Kafka

Very similar changes apply for the Slack -> Kafka flow.

. Replace the sink to target Kafka
+
Open and edit your `s2k.yaml` file.
+
The original definition remains intact except for the sink to be replaced by a _Kafka_ destination. +
Copy the sink snippet down below and paste it in your _Kamelet Binding_:
+
----
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: s2k
  annotations:
    trait.camel.apache.org/mount.configs: "secret:stage3"
    trait.camel.apache.org/mount.resources: "configmap:stage3-transform"
spec:

  source:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: slack-source
    properties:
      token:   "{{slack.token}}"
      channel: "{{slack.channel.name}}"
      delay: 2000


  steps:

  # Filter action to prevent death loops
  - ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: predicate-filter-action
    properties:
      expression: "!$.botId || $.botId == null"

  # JSON Transformation
  - ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: jslt-action
    properties:
      template: "{{transform.path:s2k.jslt}}"
----
+
```yaml
  sink:
    ref:
      kind: KafkaTopic
      apiVersion: kafka.strimzi.io/v1beta1
      name: YOUR_ROOM_NAME
```
+
[IMPORTANT]
====
Use the same sink's `name` (Kafka topic) than on your Gitter to Kafka flow.
====
+
{empty} +

. Define the JSLT transformation to the new standard data model.
+
Copy the snippet below and paste it into your new `s2k.jslt` file:
+
```
{
	"timestamp": .ts,
	"source":"slack", 
	"user": .user, 
	"text": .text
}
```
+
[NOTE]
====
We define the same common fields complying with our standard data model.
====
{empty} +

Very straightforward, nothing else to be done here. 

{empty} +








== Kafka to Gitter/Slack

The processing steps still remain essentially the same:

. receive events
. filter events
. transform events
. push events

The main differences are that we are consuming events from Kafka (step 1) and that we have to translate events (step 3) from the standard data model to the target specific model (e.g. Gitter, Slack, other)

{empty} +

=== Process overview

The diagram below applies to the data flows (2 of them) from Gitter/Slack respectively to Kafka:

image::images/processing-flow-kafka2chat.png[align="center", width=90%]


* There are 4 Kamelets in use:
+
====
A source::
consumes events from Kafka.
Two actions::
one filters messages to prevent death loops. +
one transforms events from the standard data model to Gitter/Slack.
A sink::
produces events to Gitter/Slack.
====

{empty} +



=== Kafka to Gitter

. Modify the Kamelet Binding
+
Open and edit your `k2g.yaml` file.
+
Two modifications are required:
+
--
* The source is now Kafka
* The filter should blocks self-events
--
+
Copy the corresponding snippets and replace in your _Kamelet Binding_:
+
----
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: k2g
  annotations:
    trait.camel.apache.org/mount.configs: "secret:stage3"
    trait.camel.apache.org/mount.resources: "configmap:stage3-transform"
spec:
----
+
```yaml
  source:
    ref:
      kind: KafkaTopic
      apiVersion: kafka.strimzi.io/v1beta1
      name: YOUR_ROOM_NAME
```
+
----
  steps:

  # Filter action to prevent death loops
  - ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: predicate-filter-action
    properties:
----
+
```yaml
      expression: $.source != "gitter"
```
+
----
  # JSON Transformation
  - ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: jslt-action
    properties:
      template: "{{transform.path:k2g.jslt}}"


  sink:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: gitter-sink
    properties:
      token: "{{gitter.token}}"
      room: "{{gitter.room}}"
----
+
[IMPORTANT]
====
Ensure you configure the source's `name` (Kafka topic) by replacing `YOUR_ROOM_NAME` with your chat's room name.
====
+
[NOTE]
====
The filter definition is specifically blocking events coming from _Gitter_ itself. As now Kafka sits in the middle, we are simultaneously producing and consuming Kafka events from/to Gitter, which can cause event loops. 
====
+
[NOTE]
====
The source definition in a Kamelet Binding can either be a Kamelet Source from the Catalog, or a platform resource (Kafka or KNative). The operator auto-wires the connectivity to Kafka for us.
====
{empty} +

. Define the JSLT transformation (Standard -> Gitter).
+
Copy the snippet below and paste it into your new `k2g.jslt` file:
+
```
{
    "text":"**"+.user+"@"+.source+"**: "+.text
}
```
+
[NOTE]
====
We're mapping values from the Standard data model
====
{empty} +

And that's all it takes for this first stint.

{empty} +



=== Kafka to Slack

Very similar changes apply for the Kafka -> Slack flow.


. Modify the Kamelet Binding
+
Open and edit your `k2s.yaml` file.
+
Two modifications are required:
+
--
* The source is now Kafka
* The filter should blocks self-events
--
+
Copy the corresponding snippets and replace in your _Kamelet Binding_:
+
----
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: k2s
  annotations:
    trait.camel.apache.org/mount.configs: "secret:stage3"
    trait.camel.apache.org/mount.resources: "configmap:stage3-transform"
spec:
----
+
```yaml
  source:
    ref:
      kind: KafkaTopic
      apiVersion: kafka.strimzi.io/v1beta1
      name: YOUR_ROOM_NAME
```
+
----
  steps:
    
  # Filter action to prevent death loops
  - ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: predicate-filter-action
    properties:
----
+
```yaml
      expression: $.source != "slack"
```
+
----
  - ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: jslt-action
    properties:
      template: k2s.jslt

  sink:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1
      name: slack-sink
    properties:
      token: "{{slack.token}}"
----
+
[IMPORTANT]
====
Ensure you configure the source's `name` (Kafka topic) by replacing `YOUR_ROOM_NAME` with your chat's room name.
====
+
[NOTE]
====
The filter definition is specifically blocking events coming from _Slack_ itself. As now Kafka sits in the middle, we are simultaneously producing and consuming Kafka events from/to _Slack_, which can cause event loops. 
====
+
{empty} +

. Define the JSLT transformation (Standard -> Slack).
+
Copy the snippet below and paste it into your new `k2s.jslt` file:
+
```
{
    "channel":"YOUR_ROOM_ID",
    "text":"*"+.user+"@"+.source+"*: "+.text
}
```
+
[NOTE]
====
* The field `channel` denotes the target room in Slack where messages will be pushed. Use your room ID in Slack. +
* The field `text` includes JsonPath rules extracting values from the input Standard data model.
====
{empty} +

And that's all it takes for this first stint.

{empty} +

== Deploy and test

We've covered a lot of ground. It would be normal to make mistakes. Hopefully the helper guide kept those to a minimum and, once deployed, you can see your integrations working in healthy state and delivering the expected outcome.


. Push the configuration to _OpenShift_
+
Recreate the _Secret_ and _ConfigMap_ to include both JSLTs. +
Run the following `oc` command:
+
```bash
oc create secret generic stage3 --from-file=stage3.properties
oc create cm stage3-transform --from-file=maps
```
{empty} +

. Create the kafka topic
+ 
Run the following command:
+
```bash
mkdir kafka && touch kafka/room_x.yaml
```
+
Edit your `room_x.yaml` file under the `kafka` directory. Add the following definition
+
```yaml
kind: KafkaTopic
apiVersion: kafka.strimzi.io/v1beta2
metadata:
  name: YOUR_ROOM_NAME
  labels:
    strimzi.io/cluster: my-cluster
```
+
[IMPORTANT]
====
Ensure you configure the topic's `name` by replacing `YOUR_ROOM_NAME` with your chat's room name.
====
+
Push the definition to OpenShift with the following command:
+
```bash
oc apply -f kafka/room_x.yaml
```
{empty} +

. Deploy the YAML definition containing your new Kamelet Binding
.. Run the following `oc` command to deploy the integration:
+
```bash
oc apply -f flows/g2k.yaml
oc apply -f flows/k2s.yaml
oc apply -f flows/s2k.yaml
oc apply -f flows/k2g.yaml
```
+
NOTE: Be patient, this action will take some time to complete as the operator needs to download all related dependencies, build the applications and create the images before the integrations can be deployed.

.. Wait for readyness
+
Check the deployment of all pods and their logs to ensure all is in healthy state.
+
You can run the following command to check their state:
+
```bash
oc get klb
```
+
When the pods are ready, the command should return:
+
----
NAME   PHASE   REPLICAS
g2k    Ready   1
k2g    Ready   1
k2s    Ready   1
s2k    Ready   1
----
+
{empty} +

. Send messages to test the system.
+
Go to you Slack's chat room and send a message, for example "Hello from Slack".
+
If all goes well your message should show up in Gitter.
+
image::images/stage2-msg-slack-gitter.png[align="left", width=80%]
+
{empty} +

+
{empty} +

You have now completed Stage 3 !!
